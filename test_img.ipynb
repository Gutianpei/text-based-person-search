{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "import time\n",
    "import tqdm\n",
    "import keras_contrib\n",
    "from keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from numba import jit\n",
    "from triplet_loss import batch_hard_triplet_loss\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "from numba import cuda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(json_path, dataset_path, word_model, tokenizer, time_step):\n",
    "    ''' Read from caption_test.json\n",
    "        Args:\n",
    "            json_path:  .../.../caption_test.json\n",
    "            dataset_path:  .../.../CUHK_PEDES\n",
    "        Returns:\n",
    "            ndarray\n",
    "            imgs: (2000,384,128,3)\n",
    "            ids: (2000,1)\n",
    "            caps: (2000,50,50)\n",
    "    '''\n",
    "    ids = []\n",
    "    imgs = []\n",
    "    caps = []\n",
    "    orig_caps = []\n",
    "\n",
    "    js_data = json.load(open(json_path))\n",
    "    for data in js_data:\n",
    "        if len(ids) == 1000:\n",
    "            break\n",
    "        image = cv2.imread(dataset_path + \"/imgs/\" + data[\"file_path\"])\n",
    "        image = cv2.resize(image, (128, 384))\n",
    "        image = image[:,:,::-1] #BGR to RGB\n",
    "\n",
    "        ids.append(data[\"id\"])\n",
    "        imgs.append(image)\n",
    "\n",
    "        caption = data['captions']\n",
    "        orig_caps.append(caption)\n",
    "        caps.append(tokenizer.encode(caption))\n",
    "\n",
    "    input_ids = sequence.pad_sequences(caps, maxlen=time_step, dtype='int', padding='post', truncating='post', value=0)\n",
    "    input_ids = tf.constant(input_ids)\n",
    "    attention_mask = np.where(input_ids != 0, 1, 0)\n",
    "    attention_mask = tf.constant(attention_mask)\n",
    "    outputs = word_model(input_ids, attention_mask=attention_mask)\n",
    "    caps = np.array(outputs[0])\n",
    "    \n",
    "    return np.array(ids),np.array(imgs),np.array(caps),np.array(orig_caps)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_score(mat, ids):\n",
    "    rank1 = 0\n",
    "    rank5 = 0\n",
    "    rank10 = 0\n",
    "    rank20 = 0\n",
    "    rank1_l = []\n",
    "    rank5_l = []\n",
    "    rank10_l = []\n",
    "    notfound_l = []\n",
    "\n",
    "    idx = 0 # Keep track of true id\n",
    "    print()\n",
    "    print(\"Computing RankX\")\n",
    "    for ii in range(len(mat)):\n",
    "        idx_score = mat[ii]\n",
    "        res_ids = ids[idx_score]\n",
    "        #print(res_ids[:20])\n",
    "\n",
    "        match = False\n",
    "        for i, res_id in enumerate(res_ids):\n",
    "            if res_id == ids[idx]:\n",
    "                match = True\n",
    "            if i == 0 and match:\n",
    "                rank1 += 1\n",
    "                rank1_l.append(ii)\n",
    "            if i == 9 and match:\t# we have replicated image, so here i is 9\n",
    "                rank5 += 1\n",
    "                rank5_l.append(ii)\n",
    "            if i== 19 and match:\n",
    "                rank10 += 1\n",
    "                rank10_l.append(ii)\n",
    "            if i == 39:\n",
    "                if match > 0:\n",
    "                    rank20 += 1\n",
    "                else:\n",
    "                    notfound_l.append(ii)\n",
    "                break\n",
    "        idx += 1\n",
    "\n",
    "    print(\"Rank1: \")\n",
    "    print(rank1/(idx+1))\n",
    "    print(\"Rank5: \")\n",
    "    print(rank5/(idx+1))\n",
    "    print(\"Rank10: \")\n",
    "    print(rank10/(idx+1))\n",
    "    print(\"Rank20: \")\n",
    "    print(rank20/(idx+1))\n",
    "\n",
    "    return rank1_l, rank5_l, rank10_l, notfound_l\n",
    "\n",
    "def get_models(model):\n",
    "    #print(model.summary())\n",
    "\n",
    "    print(\"Image Weights: \")\n",
    "    img_input = model.layers[0].input\n",
    "    img_output = model.layers[-3].output\n",
    "    img_model = Model(img_input, img_output)\n",
    "\n",
    "    print(\"Caption Weights: \")\n",
    "    cap_input = model.layers[-6].input\n",
    "    cap_output = model.layers[-2].output\n",
    "    cap_model = Model(cap_input, cap_output)\n",
    "    return img_model, cap_model\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "\n",
    "    label = K.flatten(y_true[:,0,0])\n",
    "\n",
    "    loss = batch_hard_triplet_loss(label, y_pred[:,1], y_pred[:,0], 0.2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Weights: \n",
      "Caption Weights: \n",
      "data and model loaded\n",
      "Computing Distance\n",
      "Matrix Ready\n",
      "\n",
      "Computing RankX\n",
      "Rank1: \n",
      "0.48451548451548454\n",
      "Rank5: \n",
      "0.7712287712287712\n",
      "Rank10: \n",
      "0.8501498501498501\n",
      "Rank20: \n",
      "0.9200799200799201\n"
     ]
    }
   ],
   "source": [
    "TIME_STEP = 100\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "word_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "ids, imgs, caps, orig_caps = get_test(\"caption_test.json\", \"../datasets/CUHK-PEDES\", word_model, tokenizer, TIME_STEP)\n",
    "\n",
    "model = load_model(\"../best_model.h5\", custom_objects={'tf': tf, 'triplet_loss': triplet_loss, 'K': K, 'InstanceNormalization':keras_contrib.layers.InstanceNormalization})\n",
    "\n",
    "img_model, cap_model = get_models(model)\t# get img path and cap path and resemble to new models\n",
    "\n",
    "print(\"data and model loaded\")\n",
    "\n",
    "print(\"Computing Distance\")\n",
    "img_out = img_model.predict(imgs)\n",
    "cap_out = cap_model.predict(caps)\n",
    "mat = cosine_similarity(cap_out, img_out) #compute cosine, output shape(6248*6248) --> (cap, img)\n",
    "sim_score = mat\n",
    "mat = np.array([np.argsort(score)[::-1] for score in mat])\n",
    "mat = mat.astype(np.int)\n",
    "ids = ids.astype(np.int)\n",
    "print(\"Matrix Ready\")\n",
    "\n",
    "rank1, rank5, rank10, notfound = compute_score(mat, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n",
      "287\n",
      "79\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "rank10 = list(set(rank10) - set(rank5))\n",
    "rank5 = list(set(rank5) - set(rank1))\n",
    "\n",
    "for i in [rank1, rank5, rank10, notfound]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_imgs = rank10[:20]\n",
    "columns = 10\n",
    "rows = 1\n",
    "\n",
    "for kk in print_imgs:\n",
    "    plt.axis('off')\n",
    "    plt.imshow(imgs[kk])\n",
    "    print(ids[kk])\n",
    "    print(orig_caps[kk])\n",
    "    temp = mat[kk]\n",
    "    temp = temp[:columns*rows*2]\n",
    "    temp_imgs = imgs[temp]\n",
    "    temp_sims = sim_score[kk][temp]\n",
    "    temp_ids = ids[temp]\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(1, columns*rows +1):\n",
    "        sub_temp = fig.add_subplot(rows, columns, i)\n",
    "        sub_temp.title.set_text(str(temp_ids[i]) + '$' + str(temp_sims[i])[:5])\n",
    "        plt.imshow(temp_imgs[(i-1)*2])\n",
    "        plt.axis('off')\n",
    "        #plt.tight_layout(pad=9.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
